{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lta36/RL/blob/main/RC2PyTorch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43njs_j6fmh6"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EElLdHGGvpdY"
      },
      "source": [
        "# Using GPU Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWF3grGxvpBC",
        "outputId": "cbeb0022-a0d5-4328-f4b7-10594fefe379"
      },
      "source": [
        "# change the runtime type to 'gpu' to return true\n",
        "if (torch.cuda.is_available()):\n",
        "  print('CUDA AVAILABLE')\n",
        "  print('Number of devices {}'.format(torch.cuda.device_count()))\n",
        "  print('First Device name {}'.format(torch.cuda.get_device_name(0)))\n",
        "\n",
        "else:\n",
        "  print('NO GPU DETECTED')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA AVAILABLE\n",
            "Number of devices 1\n",
            "First Device name Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxpnz7KUxIAX",
        "outputId": "ad83740b-af5a-4951-f40a-9ff9aabc8eca"
      },
      "source": [
        "# initialize a 'device' for ourselves\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# multiple ways to utilize the devices\n",
        "a = torch.normal(0, 1, size=(1000,1000))\n",
        "b = torch.normal(0, 1, size=(1000,1000))\n",
        "\n",
        "a_cuda = a.to(device)\n",
        "b_cuda = b.cuda()\n",
        "\n",
        "# a, b no both in GPU memory\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "c_cuda = torch.normal(0, 1, a_cuda.shape).cuda()\n",
        "# start.record()\n",
        "start = time.time()\n",
        "for i in range(300):\n",
        "  c_cuda = a_cuda + c_cuda@b_cuda\n",
        "# end.record()\n",
        "torch.cuda.synchronize() # cuda operations are asynchronous, we must ensure completion\n",
        "\n",
        "print('GPU TIME {}'.format(time.time() - start))\n",
        "\n",
        "# bring them back out of gpu memory (for non tensor operations)\n",
        "d = a.cpu()\n",
        "e = b.cpu()\n",
        "\n",
        "c = torch.normal(0, 1, a.shape)\n",
        "start = time.time()\n",
        "for i in range(300):\n",
        "  c = d + c @ e\n",
        "\n",
        "print('CPU TIME {}'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU TIME 0.4623076915740967\n",
            "CPU TIME 8.317511558532715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "bniGnK1I19lM",
        "outputId": "38cc9023-6e6e-4591-e80a-944f34c3796c"
      },
      "source": [
        "# Be careful that all tnesors are on the same device or else...\n",
        "f = torch.normal(0, 1, (10, 10)).cuda()\n",
        "g = torch.normal(0, 1, (10, 10))\n",
        "f + g"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c88deeabd507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4X5iydffx-r",
        "outputId": "cfc1cdf6-c3f5-48ab-c58f-f740c5c8cecf"
      },
      "source": [
        "\n",
        "X = torch.normal(0, 1, size=(10,10))\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2125,  0.1101,  0.1592,  0.1464, -0.7851, -0.5717, -1.0404,  0.3948,\n",
              "         -0.8741, -0.1540],\n",
              "        [-0.5060, -0.2522,  0.8530, -0.7845, -0.5615,  0.4727,  0.7359, -0.6029,\n",
              "         -0.1937, -1.7146],\n",
              "        [ 0.3140,  0.6259,  1.0818,  0.8351, -1.9414,  0.7980, -0.6748, -0.4471,\n",
              "         -1.7709, -0.5060],\n",
              "        [-3.2152,  1.9721, -0.1604, -0.9541, -0.3643,  0.6880,  1.0778,  0.8099,\n",
              "          1.6711,  0.5627],\n",
              "        [ 0.9598, -0.3241, -1.6854, -0.1205, -0.9592, -0.3132,  0.7367, -0.1958,\n",
              "          1.1983,  1.2918],\n",
              "        [ 0.3128,  2.3083,  2.5882,  0.4949, -0.3814,  0.6669, -0.4542, -0.1096,\n",
              "         -1.5927,  0.1255],\n",
              "        [-0.6593,  1.0749, -0.2221, -0.5929,  0.1506, -0.1506, -0.9415, -0.0319,\n",
              "          0.2815, -1.4177],\n",
              "        [ 1.3241, -0.7738,  0.7319,  0.7543, -0.8877,  1.1602, -1.6581,  0.0907,\n",
              "          1.2870, -0.0309],\n",
              "        [-1.5790,  1.4794,  1.1337, -2.2275,  0.1356, -0.8547,  0.4506, -0.1759,\n",
              "         -2.1433, -0.1995],\n",
              "        [ 0.4458,  0.7481,  1.8436,  1.4305, -2.0403,  0.8331, -0.8830,  0.2078,\n",
              "         -0.1519, -2.0062]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddh8SzVbfyIo",
        "outputId": "aabb2d99-7335-45a6-9d17-f8e2b8d33459"
      },
      "source": [
        "# Repeat from TF tutorial\n",
        "Y = np.eye(2)[np.random.choice(2,10)]\n",
        "Y = torch.tensor(Y, dtype=torch.float32)\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qajESEUWhCQB"
      },
      "source": [
        "model1 = torch.nn.Sequential(\n",
        "    torch.nn.Linear(10, 5),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(5, 5),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(5, 2),\n",
        "    torch.nn.Softmax()\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rScGTKQOs-oQ"
      },
      "source": [
        "# initialize loss criterion and optimzier as separate objects\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model1.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqu3Gymds-x-",
        "outputId": "7b9f2d43-9591-4f73-9711-ec7e86436012"
      },
      "source": [
        "# run training epoch by epoch\n",
        "for epoch in range(10):\n",
        "  epoch_loss = 0.0\n",
        "  correct = 0\n",
        "  for i, data in enumerate(zip(X, Y)):\n",
        "    input, label = data\n",
        "\n",
        "    #reset the stored parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #run model\n",
        "    output = model1(input)\n",
        "    loss = criterion(output, label)\n",
        "\n",
        "    #optimization step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss\n",
        "    classified_out = torch.round(output)\n",
        "\n",
        "    if (torch.equal(classified_out, label)):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/Y.shape[0]\n",
        "  print('EPOCH [{}] loss [{}] acc [{}]'.format(epoch, loss, acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH [0] loss [0.12016613781452179] acc [0.7]\n",
            "EPOCH [1] loss [0.11737425625324249] acc [0.7]\n",
            "EPOCH [2] loss [0.11467073857784271] acc [0.7]\n",
            "EPOCH [3] loss [0.11220872402191162] acc [0.7]\n",
            "EPOCH [4] loss [0.1098659336566925] acc [0.7]\n",
            "EPOCH [5] loss [0.10750657320022583] acc [0.7]\n",
            "EPOCH [6] loss [0.10507810860872269] acc [0.7]\n",
            "EPOCH [7] loss [0.10255584120750427] acc [0.7]\n",
            "EPOCH [8] loss [0.09992557764053345] acc [0.7]\n",
            "EPOCH [9] loss [0.09717822074890137] acc [0.7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9vBYMl9zQ8g"
      },
      "source": [
        "\n",
        "# Customize your own NN!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdDhaY_YzEDg"
      },
      "source": [
        "class NeuralNet(torch.nn.Module):\n",
        "  def __init__(self, output_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = torch.nn.Linear(10, 5)\n",
        "    self.activation1 = torch.nn.ReLU()\n",
        "    self.linear2 = torch.nn.Linear(5, 5)\n",
        "    self.activation2 = torch.nn.ReLU()\n",
        "\n",
        "    self.output_layer = torch.nn.Linear(5, output_size)\n",
        "    self.output_activation = torch.nn.Softmax()\n",
        "\n",
        "    #initialize random weights\n",
        "    # torch.nn.init.normal_(self.linear1.weight, mean=0.0, std=1.0)\n",
        "    # torch.nn.init.normal_(self.linear2.weight, mean=0.0, std=1.0)\n",
        "    # torch.nn.init.normal_(self.output_layer.weight, mean=0.0, std=1.0)\\\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    x = self.activation1(self.linear1(inputs))\n",
        "    x = self.activation2(self.linear2(x))\n",
        "\n",
        "    x = self.output_activation(self.output_layer(x))\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfO8zWAv08iO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a934582-555e-45d3-f789-4c5aa3cd09e1"
      },
      "source": [
        "model2 = NeuralNet(2)\n",
        "print(list(model2.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('linear1.weight', Parameter containing:\n",
            "tensor([[-0.1250,  0.1025,  0.0731,  0.2993, -0.1616,  0.2586,  0.0217, -0.2611,\n",
            "         -0.0079, -0.0581],\n",
            "        [-0.1873,  0.2927,  0.1852,  0.0815,  0.0603,  0.1632, -0.1424,  0.0649,\n",
            "          0.1465,  0.1581],\n",
            "        [ 0.2487,  0.1358,  0.2726, -0.2619, -0.3089, -0.1049,  0.1554, -0.2690,\n",
            "          0.2001, -0.0211],\n",
            "        [ 0.0308, -0.2634, -0.0473,  0.2982,  0.1203,  0.0688,  0.0506, -0.0553,\n",
            "          0.2722,  0.0537],\n",
            "        [-0.0855, -0.0745, -0.2687,  0.0787,  0.0972, -0.2620,  0.0157, -0.0204,\n",
            "         -0.2964,  0.0421]], requires_grad=True)), ('linear1.bias', Parameter containing:\n",
            "tensor([ 0.2659, -0.2629, -0.0035, -0.2377,  0.2223], requires_grad=True)), ('linear2.weight', Parameter containing:\n",
            "tensor([[ 0.0641,  0.0524,  0.0725,  0.0139, -0.0973],\n",
            "        [-0.1984, -0.3504, -0.0109,  0.1071,  0.4279],\n",
            "        [ 0.1059, -0.3904, -0.3289,  0.1990, -0.4020],\n",
            "        [-0.2206,  0.0885, -0.2971, -0.2692,  0.3375],\n",
            "        [ 0.2613,  0.1395, -0.2066,  0.3715,  0.4165]], requires_grad=True)), ('linear2.bias', Parameter containing:\n",
            "tensor([ 0.3916, -0.0935,  0.1899, -0.0994,  0.2027], requires_grad=True)), ('output_layer.weight', Parameter containing:\n",
            "tensor([[ 0.1734, -0.0730, -0.4222, -0.2795, -0.2090],\n",
            "        [ 0.3922,  0.1837,  0.0838,  0.1017, -0.0026]], requires_grad=True)), ('output_layer.bias', Parameter containing:\n",
            "tensor([-0.1829,  0.0064], requires_grad=True))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxssCJ_S08qU",
        "outputId": "4fadf538-b9fe-4716-a9a8-ffaf18a53f3b"
      },
      "source": [
        "model2(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5727, 0.4273],\n",
              "        [0.6046, 0.3954],\n",
              "        [0.5810, 0.4190],\n",
              "        [0.5618, 0.4382],\n",
              "        [0.5380, 0.4620],\n",
              "        [0.6006, 0.3994],\n",
              "        [0.5993, 0.4007],\n",
              "        [0.5983, 0.4017],\n",
              "        [0.6291, 0.3709],\n",
              "        [0.5436, 0.4564]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}